# Validation
The process of validation typically means the comparison of a model or a developed EO-product to reference data where the agreement is described by validation metrics. The validation of EO products is essential to avoid misinterpretation or error propagation when the data are used for e.g. area quantification, subsequent modelling or even planning purposes for example in the context of nature conservation or risk assessment. Yet, the validation of EO products is a challenging task. In this tutorial, we will explain how to derive validation metrics and how to interpret them. Our primary focus will be on the challenges and limits in the process of accuracy assessment especially with regard to large-scale (global) mapping products based on EO.

## Learning objectives
- Understand why validation is important
- Get to know typical validation strategies in EO
<!-- - Understand where uncertainties appear in a workflow by design
- The validation of large scale mapping products -->
- Know common challenges and limits of accuracy assessment
- Validate some pixels of the snow cover area map




<!--## Critically Analyse a workflow
- Identify sources of uncertainty in the applied workflow
- Process graph with pop-ups of sources of uncertainties
- Strategies of how to improve-->

<!--further uncertainties: change of support, quality of the reference data-->

## Typical validation approaches

### Reference data
Reference data for EO are typically acquired via field surveys or via visual expert assessment of the underlying EO data. Reference data are not only used for validation but also required to train the model if the EO-product is based on predictions made by a data-driven model. Therefore, when speaking of reference data, we broadly distinguish between training and validation data. The training data are used to develop the model and the validation data are used to assess the quality of the generated product (i.e. of the predictions made my the model).


### Model validation and map validation
Many EO products are generated based on data-driven models. These might be simple rule based models but most commonly, products are generated using complex machine learning models. In the process of generating such EO product there are at least two validation steps that we need to differentiate: model validation and map validation. During model validation we assess the quality of the model to predict the target variable (e.g. snow cover) from the EO data (e.g. optical satellite imagery). This step is often done via cross-validation, where the training data are split into several folds. We then iteratively retain one fold and use these held-back data to test the performance of the model when predicting unseen data. Cross-validation often involves tuning of the model (hyperparameters, variable selection) to identify the model that is best able to predict the held-back data.
If (and only if) the training data and the derived cross-validation folds are representative for the prediction area (see discussion later), the cross-validation performance may be used as an indicator for the map accuracy. 

To  measure the map accuracy, a probability sample of the prediction area is required. This might be a random sample of the entire area that is used to describe the fit between the prediction (i.e. the map) and the reference. 
In many scientific publications, this is rarely done and the cross-validation performance is communicated as the only indicator for map accuracy. We will outline the risks below.

### validation strategies
How do we split the reference data?

### validation metrics
Depending on the target variable
Regression models, RMSE, R2 most frequently used ones. For classification accuracy or for binary classifications the area under the curve. However many more metrics beyond scope of the tutorial. Link to XX.


## Validation in the absence of a probability sample
Above strategies for the case of a random sample. However, for many tasks that does not exist.
Especially for large scale mapping approaches such as global applications. Reference data usually come from large data bases of e.g. soil profiles or vegetation surveys and are highly clustered in areas that were intesively studied or easily accessible. Other areas completely lack reference data. 

![Typical distributions of reference data](assets/distribution_map.png)
Figure 1: Comparison between 1000 randomly sampled reference data (left) and a highly clustered sample of the same size (right) that is typical for many environmental data sets. 


model accuracy should resemble map accuracy


This raises the question on how to use reference data for cross-validation as well as for map accuracy assessment. If purpose of model is clear the CV should already indicate map accuracy by using reference data that are representative for the prediction area.


<!--- Ways of doing proper accuracy assessment (methods)
- How to express the certainty of the map/pixels (measures)
- Taking into account the accuracy of the validation data
- How to properly state the limitations of a map (what is still not quantifiable after the validation)
- How to publish the validation results with the main product-->

## Limits to accuracy assessment
Looking at reference data see that large gaps. Making predictions for these areas is technically no problem but is this a good idea? It depends on teh question whether our model was trained for these areas. Typical case: reference data for vegetation traits were only sampled in low elevations. Would this model also be applicable to high elevations? This is especially problematic when elevation is a predictor in the model, i.e. the predictions are based on elevation. An other example would be downscaling climate variables into the high mountains which might noit have been considered in the model training. This is especially problem, when machine learning models are used extrapolation abilities are poor. When making predictions, the model is forced to make preci tion into unknown areas hence the predictions for areas outside this range shoudl be regarded as highly uncertain.



<!--further uncertainties: change of support, quality of the reference data
- Producing Global/Continental data sets, the spatial distribution of sample/training/validation points and extrapolation into the unknown (e.g. trained model into amazonas/sahara, downscaling climate variables into the high mountains) -> S14Science in Amazonas (ground truth in remote locations)-->



#### Video: The validation of large scale mapping products


[![Machine learning-based maps of the environment:
challenges of extrapolation and overfitting](https://doi.org/10.5446/59412)](https://doi.org/10.5446/59412)


## Notes
Here we were mainly focusing on validation strategies. We often refer to these reference data as "ground truth data", because we make the assumption that they represent the truth. Note that other aspects like sampling units, colocation etc are relevant as well (e.g. Loew et al 2017). Taking into account the accuracy of the validation data...

## Exercise
- Validate some pixels of the SCA map produced before
- Use station data from ClirSnow Project: https://clirsnow.netlify.app/dash-results/dash-climatology.html#explore-climatic-boundaries
  - There seem to be 4-6 stations in the catchment
- Express the accuracy correctly, publish it with the result (metadata or layer)

## Quiz
- Dropdown select the accuracy the SCA map at a station (result of the exercise)
- 
-
-
-
- 

## References
- https://www.nature.com/articles/s41467-022-29838-9
- https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2017RG000562
